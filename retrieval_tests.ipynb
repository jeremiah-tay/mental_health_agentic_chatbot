{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16c89852",
   "metadata": {},
   "source": [
    "# Retrieval Tests â€” Supabase pgvector + OpenAI embeddings\n",
    "\n",
    "This notebook walks through:\n",
    "1. Loading environment variables (from a local `.env`).\n",
    "2. Connecting to Supabase (service role key) and OpenAI.\n",
    "3. Extracting text from PDFs.\n",
    "4. Chunking text, generating embeddings, and uploading to Supabase.\n",
    "5. Running similarity queries via the `match_documents` RPC.\n",
    "\n",
    "**Important:**  \n",
    "- Do **NOT** store your real keys in this notebook. Keep them in a local `.env` file in the project root (listed in `.gitignore`).  \n",
    "- Run this notebook **from your project root** (the folder that contains `backend/`, `tools/`, `migrations/`, `pdf/`, etc.).  \n",
    "- Make sure you've already run the SQL migrations in Supabase (the `migrations/` files).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b85a664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once).\n",
    "# You may prefer to run `pip install -r requirements.txt` in your shell instead.\n",
    "!pip install supabase python-dotenv pypdf openai tqdm --quiet\n",
    "print('Packages install attempted (quiet mode). If this cell errors, install packages manually in your environment.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ceb57064",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables from .env in the project root.\n",
    "from dotenv import load_dotenv\n",
    "import os, sys\n",
    "\n",
    "# ensure we run from project root (user should start notebook from project root)\n",
    "print(\"Current working directory:\", os.getcwd())\n",
    "\n",
    "load_dotenv()  # reads .env\n",
    "\n",
    "SUPABASE_URL = os.environ.get(\"SUPABASE_URL\")\n",
    "SUPABASE_KEY = os.environ.get(\"SUPABASE_SERVICE_ROLE_KEY\")\n",
    "OPENAI_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "OPENAI_MODEL = os.environ.get(\"OPENAI_EMBEDDING_MODEL\", \"text-embedding-3-small\")\n",
    "\n",
    "print(\"SUPABASE_URL set?:\", bool(SUPABASE_URL))\n",
    "print(\"SUPABASE_KEY set?:\", bool(SUPABASE_KEY))\n",
    "print(\"OPENAI_KEY set?:\", bool(OPENAI_KEY))\n",
    "print(\"OPENAI_MODEL:\", OPENAI_MODEL)\n",
    "\n",
    "if not (SUPABASE_URL and SUPABASE_KEY and OPENAI_KEY):\n",
    "    raise RuntimeError(\"Make sure your .env contains SUPABASE_URL, SUPABASE_SERVICE_ROLE_KEY, and OPENAI_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af514e02",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Supabase and OpenAI clients\n",
    "from supabase import create_client\n",
    "from openai import OpenAI\n",
    "\n",
    "sb = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "openai = OpenAI(api_key=OPENAI_KEY)\n",
    "\n",
    "print('Supabase and OpenAI clients created.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2f3e290",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PDF text extraction helper (uses pypdf)\n",
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(path: str) -> str:\n",
    "    reader = PdfReader(path)\n",
    "    pages = []\n",
    "    for page in reader.pages:\n",
    "        text = page.extract_text() or ''\n",
    "        pages.append(text)\n",
    "    return '\\n'.join(pages).strip()\n",
    "\n",
    "# Quick local test: list pdf files if folder exists\n",
    "import glob, os\n",
    "pdf_files = glob.glob('pdf/*.pdf')\n",
    "print('Found PDFs in pdf/:', pdf_files[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a588880a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple chunker (paragraph-aware)\n",
    "def chunk_text(text: str, max_chars: int = 1500):\n",
    "    paragraphs = [p.strip() for p in text.split('\\n\\n') if p.strip()]\n",
    "    out = []\n",
    "    cur = \"\"\n",
    "    for p in paragraphs:\n",
    "        if len(cur) + len(p) + 2 <= max_chars:\n",
    "            cur = (cur + \"\\n\\n\" + p).strip()\n",
    "        else:\n",
    "            if cur: out.append(cur)\n",
    "            cur = p\n",
    "    if cur: out.append(cur)\n",
    "    return out\n",
    "\n",
    "# quick sanity\n",
    "print('Chunker ready. Example chunk counts for first PDF (if any):')\n",
    "if pdf_files:\n",
    "    sample_text = extract_text_from_pdf(pdf_files[0])\n",
    "    print('chars:', len(sample_text), 'chunks:', len(chunk_text(sample_text, max_chars=1500)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f585ff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embedding helper using OpenAI client\n",
    "def embed_text(text: str, model: str = OPENAI_MODEL):\n",
    "    # OpenAI client returns embeddings via .embeddings.create()\n",
    "    res = openai.embeddings.create(model=model, input=text)\n",
    "    emb = res.data[0].embedding\n",
    "    return emb\n",
    "\n",
    "# Quick sanity: do NOT call on long texts here automatically.\n",
    "print('Embedding helper ready (will call OpenAI when used).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba66c233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload chunks to Supabase table 'documents'\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def upload_chunks_to_supabase(chunks, source='notebook_upload', pause_sec=0.2, max_batch=50):\n",
    "    '''\n",
    "    Inserts chunks into Supabase documents table.\n",
    "    - chunks: list of text chunks\n",
    "    - source: string identifying the source\n",
    "    - pause_sec: short pause between batches to avoid rate limits\n",
    "    - max_batch: number of rows per insert\n",
    "    '''\n",
    "    rows = []\n",
    "    inserted = 0\n",
    "    for i, chunk in enumerate(tqdm(chunks, desc='embedding chunks')):\n",
    "        emb = embed_text(chunk)\n",
    "        rows.append({\n",
    "            'source': source,\n",
    "            'content': chunk,\n",
    "            'chunk_index': i,\n",
    "            'embedding': emb\n",
    "        })\n",
    "        # insert in batches\n",
    "        if len(rows) >= max_batch:\n",
    "            res = sb.table('documents').insert(rows).execute()\n",
    "            if getattr(res, 'error', None):\n",
    "                print('Insert error:', res.error)\n",
    "            else:\n",
    "                inserted += len(rows)\n",
    "            rows = []\n",
    "            time.sleep(pause_sec)\n",
    "    # insert remaining\n",
    "    if rows:\n",
    "        res = sb.table('documents').insert(rows).execute()\n",
    "        if getattr(res, 'error', None):\n",
    "            print('Insert error:', res.error)\n",
    "        else:\n",
    "            inserted += len(rows)\n",
    "    return inserted\n",
    "\n",
    "print('Upload function ready (will write to your Supabase documents table).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55dd8f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch-upload all PDFs from pdf/ folder (run this cell to process all PDFs)\n",
    "import glob, os\n",
    "\n",
    "def process_all_pdfs(pdf_dir='pdf', chunk_size=1500):\n",
    "    files = glob.glob(os.path.join(pdf_dir, '*.pdf'))\n",
    "    if not files:\n",
    "        print('No PDF files found in', pdf_dir)\n",
    "        return\n",
    "    total_inserted = 0\n",
    "    for f in files:\n",
    "        print('\\nProcessing', f)\n",
    "        txt = extract_text_from_pdf(f)\n",
    "        if not txt:\n",
    "            print('  - no extractable text (maybe scanned PDF). Skipping.')\n",
    "            continue\n",
    "        chunks = chunk_text(txt, max_chars=chunk_size)\n",
    "        print(f'  - chunks: {len(chunks)}')\n",
    "        inserted = upload_chunks_to_supabase(chunks, source=os.path.basename(f))\n",
    "        print(f'  - inserted {inserted}')\n",
    "        total_inserted += inserted\n",
    "    print('\\nDone. Total inserted:', total_inserted)\n",
    "\n",
    "# To run: uncomment the next line and execute the cell\n",
    "# process_all_pdfs('pdf', chunk_size=1500)\n",
    "print('To run batch upload, call process_all_pdfs(\"pdf\").')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216ff483",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick check: fetch some rows from documents\n",
    "print('Fetching up to 5 documents from Supabase (for inspection):')\n",
    "res = sb.table('documents').select('id, source, content, chunk_index, created_at').limit(5).execute()\n",
    "if getattr(res, 'error', None):\n",
    "    print('Error fetching rows:', res.error)\n",
    "else:\n",
    "    rows = getattr(res, 'data', [])\n",
    "    for r in rows:\n",
    "        print('\\nID:', r.get('id'), 'source:', r.get('source'), 'chunk_index:', r.get('chunk_index'))\n",
    "        print('snippet:', (r.get('content') or '')[:200].replace('\\n', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b8e92d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieval test: embed a query and call the match_documents RPC\n",
    "def retrieve(query: str, k: int = 5):\n",
    "    q_emb = embed_text(query)\n",
    "    res = sb.rpc('match_documents', {'query_embedding': q_emb, 'match_limit': k}).execute()\n",
    "    if getattr(res, 'error', None):\n",
    "        raise Exception(res.error)\n",
    "    return res.data\n",
    "\n",
    "# Example query - change to something relevant to your PDFs\n",
    "test_query = 'How to calm down during a panic attack?'\n",
    "print('Query:', test_query)\n",
    "hits = retrieve(test_query, k=5)\n",
    "print('\\nTop hits:')\n",
    "for h in hits:\n",
    "    print('distance:', h.get('distance'), 'source:', h.get('source'))\n",
    "    print('snippet:', (h.get('content') or '')[:300].replace('\\n',' '), '\\n---')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0b05a8d",
   "metadata": {},
   "source": [
    "## Troubleshooting & Notes\n",
    "\n",
    "- **No text extracted from PDF**: many scanned PDFs are images â€” use OCR (Tesseract) or get the text source.  \n",
    "- **Dimension mismatch errors on insert**: ensure your OpenAI embedding model returns a vector length matching `vector(1536)` in your DB. If you used a different model, change the SQL migration and function accordingly.  \n",
    "- **Rate limits / slow uploads**: the notebook uses a tiny pause and batch inserts; increase `pause_sec` or reduce parallel requests if you hit OpenAI rate limits.  \n",
    "- **Running the notebook**: from your project root run `jupyter notebook` or `jupyter lab`, open `retrieval_tests.ipynb`, and run cells in order.  \n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
